# Coreference Resolution with multi-head and NER

best {
  # Computation limits
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 0.5

  # Model hyperparameters
  char_embedding_size = 50
  char_lstm_size = 100
  contextualization_size = 200
  contextualization_layers = 2
  ffnn_size = 100
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 20
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  coarse_to_fine = true
  refinement_sharing = true

  # Attention parameters
  use_multihead_attention = true
  num_attention_heads = 4
  attention_size = 64
  attention_dropout = 0.1
  use_pre_lstm_attention = true

  # NER-specific parameters
  ner_lstm_size = 200
  ner_use_crf = true
  ner_loss_weight = 0.1
  max_ner_span_width = 10
  ner_embedding_size = 20
  ner_types = ["PER", "ORG", "LOC", "MISC"]

  # Learning hyperparameters
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.3
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 1.00
  decay_frequency = 100
  ema_decay = 0.9999

  # Loss configuration
  eval_frequency = 500
  report_frequency = 100
  max_step = 10000
  log_root = logs
  gold_loss = false
  b3_loss = false
  mention_loss = false
  antecedent_loss = true

  # Entity handling
  entity_equalization = false
  antecedent_averaging = true
  use_cluster_size = true
  entity_average = false

  # Mention handling
  use_gold_mentions = true
  include_singletons = true
  eval_for_mentions = false

}

# Amharic base configuration
am_base = ${best} {
  # Path configurations
  char_vocab_path = "char_vocab.amharic.txt"
  
  
  # Embeddings
  head_embeddings {
    path = "cc.am.300.vec"
    size = 300
  }
  context_embeddings {
    path = "cc.am.300.vec"
    size = 300
  }
  
  # Corpus paths
  train_path = "train.amharic.jsonlines"
  eval_path = "dev.amharic.jsonlines"
  
  # Language-specific
  genres = ["st"] # Amharic
  include_singletons = true
  save_frequency = 100
  
  # Attention tuning for Amharic
  num_attention_heads = 4
  attention_head_size = 330
  use_transformer_context = false
  dropout = 0.0
  
  # NER adjustments for Amharic
  ner_lstm_size = 200 
  ner_use_crf = true
  ner_loss_weight = 0.2 
}

# Coreference-focused configurations
am_coref = ${am_base} {
  use_gold_mentions = true
  eval_for_mentions = false
  use_attention_in_span = true
  use_attention_in_antecedent = true
  ner_loss_weight = 0.1 
}

# Mention detection-focused configurations
am_ment = ${am_base} {
  use_gold_mentions = true
  eval_for_mentions = true
  mention_loss = true
  antecedent_loss = false
  top_span_ratio = 0.25
  use_attention_in_span = true
  use_attention_in_antecedent = false
  ner_loss_weight = 0.3 
}

# Combined mention and coreference configurations
am_mentcoref = ${am_base} {
  use_gold_mentions = true
  eval_for_mentions = false
  use_attention_in_span = true
  use_attention_in_antecedent = true
  ner_loss_weight = 0.15 # Balanced weight
}

# Training configurations with filtered embeddings
train_am_coref = ${am_coref} {
  head_embeddings {
    path = "cc.am.300.vec.filtered"
    size = 300
  }
  context_embeddings {
    path = "cc.am.300.vec.filtered"
    size = 300
  }
  
  use_multihead_attention = true
  num_attention_heads = 4
  attention_size = 64
  ner_loss_weight = 0.1 
}

train_am_ment = ${am_ment} {
  head_embeddings {
    path = "cc.am.300.vec.filtered"
    size = 300
  }
  context_embeddings {
    path = "cc.am.300.vec.filtered"
    size = 300
  }
  
  use_multihead_attention = false
  ner_loss_weight = 0.3 
}

train_am_mentcoref = ${am_mentcoref} {
  head_embeddings {
    path = "cc.am.300.vec.filtered"
    size = 300
  }
  context_embeddings {
    path = "cc.am.300.vec.filtered"
    size = 300
  }
  
  use_multihead_attention = true
  num_attention_heads = 4
  attention_size = 64
  learning_rate = 0.0005
  max_step = 10000
  ner_loss_weight = 0.15 
}

# Experimental configurations
am_attention_heavy = ${am_base} {
  use_multihead_attention = true
  num_attention_heads = 4
  attention_size = 64
  attention_dropout = 0.2
  coref_depth = 3
  contextualization_layers = 3
  ner_lstm_size = 300 
}

am_fast = ${am_base} {
  use_multihead_attention = false
  coref_depth = 1
  contextualization_layers = 1
  max_top_antecedents = 50
  ner_lstm_size = 128 
  ner_use_crf = false 
}

# NER-focused configuration
am_ner_focused = ${am_base} {
  use_gold_mentions = true
  eval_for_mentions = false
  ner_loss_weight = 0.5 
  mention_loss = true
  antecedent_loss = true
  max_ner_span_width = 15
}
